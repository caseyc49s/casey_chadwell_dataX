{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![data-x](http://oi64.tinypic.com/o858n4.jpg)\n",
    "\n",
    "\n",
    "# Homework 8\n",
    "Sentiment Analysis on IMDB movie reviews\n",
    "\n",
    "Reference: https://github.com/ikhlaqsidhu/data-x/blob/master/07-tools-webscraping-crawling-nlp-sentiment-sc1t/notebook-nlp-sentiment-analysis-imdb-afo_v1.ipynb\n",
    "\n",
    "https://github.com/ikhlaqsidhu/data-x/blob/master/07a-tools-nlp-sentiment_add_missing_si/NLP1-slides_v2_afo.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name - Casey Chadwell\n",
    "\n",
    "## SID - 3033291861"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As you go through the notebook, you will encounter these main steps in the code: \n",
    "1. Reading of file labeledTrainData.tsv from data folder in a dataframe `train`.\n",
    "2. A function review_cleaner(train['review'],lemmatize,stem) which cleans the reviews in the input file.\n",
    "3. A function train_predict_sentiment(cleaned_reviews, y=train[\"sentiment\"],ngram=1,max_features=1000\n",
    "4. You will see a model has been trained on unigrams of the reviews without lemmatizing and stemming.\n",
    "5. Your task is in 5.TODO section.\n",
    "\n",
    "\n",
    "\n",
    "Run the cells below-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data set\n",
    "\n",
    "The labeled training data set consists of 25,000 IMDB movie reviews. There is also an unlabeled test set with 25,000 IMDB movie reviews. The sentiment of the reviews are binary, meaning an IMDB rating < 5 results in a sentiment score of 0, and a rating >=7 have a sentiment score of 1 (no reviews with score 5 or 6 are included in the analysis). No individual movie has more than 30 reviews.\n",
    "\n",
    "## File description\n",
    "\n",
    "* **labeledTrainData** - The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id, sentiment, and text for each review. \n",
    "\n",
    "* **testData** - The unlabeled test set. 25,000 rows containing an id, and text for each review. \n",
    "\n",
    "## Data columns\n",
    "* **id** - Unique ID of each review\n",
    "* **sentiment** - Sentiment of the review; 1 for positive reviews and 0 for negative reviews\n",
    "* **review** - Text of the review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data set statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd       \n",
    "train = pd.read_csv(\"data/labeledTrainData.tsv\", header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)\n",
    "# train.shape should be (25000,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import bs4 as bs\n",
    "import nltk\n",
    "\n",
    "# nltk.download('all')\n",
    "from nltk.tokenize import sent_tokenize # tokenizes sentences\n",
    "import re\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='sec3'></div>\n",
    "##  2.Preparing the data set for classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a function called `review_cleaner` that reads in a review and:\n",
    "\n",
    "- Removes HTML tags (using beautifulsoup)\n",
    "- **Extract emoticons (emotion symbols, aka smileys :D )**\n",
    "- Removes non-letters (using regular expression)\n",
    "- Converts all words to lowercase letters and tokenizes them (using .split() method on the review strings, so that every word in the review is an element in a list)\n",
    "- Removes all the English stopwords from the list of movie review words\n",
    "- Join the words back into one string seperated by space, append the emoticons to the end\n",
    "\n",
    "**NOTE: Transform the list of stopwords to a set before removing the stopwords. I.e. assign `eng_stopwords = set(stopwords.words(\"english\"))`. Use the set to look up stopwords. This will speed up the computations A LOT (Python is much quicker when searching a set than a list).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def review_cleaner(reviews,lemmatize=True,stem=False):\n",
    "    '''\n",
    "    Clean and preprocess a review.\n",
    "\n",
    "    1. Remove HTML tags\n",
    "    2. Use regex to remove all special characters (only keep letters)\n",
    "    3. Make strings to lower case and tokenize / word split reviews\n",
    "    4. Remove English stopwords\n",
    "    5. Rejoin to one string\n",
    "    '''\n",
    "    ps = PorterStemmer()\n",
    "    wnl = WordNetLemmatizer()\n",
    "        #1. Remove HTML tags\n",
    "    \n",
    "    cleaned_reviews=[]\n",
    "    for i,review in enumerate(train['review']): \n",
    "    ## ennumerate returns pairs of (i, ele) for each i in review\n",
    "    # print progress\n",
    "        if( (i+1)%500 == 0 ):\n",
    "            print(\"Done with %d reviews\" %(i+1))\n",
    "        ## this is nice because it keeps from stoping if it takes too long\n",
    "        review = bs.BeautifulSoup(review).text\n",
    "\n",
    "        #2. Use regex to find emoticons\n",
    "        emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', review)\n",
    "        \n",
    "        ## From the python docs:\n",
    "        \"\"\" (?:...)\n",
    "        A non-capturing version of regular parentheses. Matches whatever regular expression is inside the parentheses, \n",
    "        but the substring matched by the group cannot be retrieved after performing a match or referenced later in the pattern.\n",
    "        \"\"\"\n",
    "        ## but when '?' is used after ')', it indicates that 0 or 1 of the previous expression is there\n",
    "        ## So this reads: ':' or ';' or '=' followed possibly by '-' then either ')' or '(' or 'D' or 'P'\n",
    "\n",
    "        #3. Remove punctuation\n",
    "        review = re.sub(\"[^a-zA-Z]\", \" \",review)\n",
    "        ## substitutes all elements that are not upper or lower case letters with a space\n",
    "\n",
    "        #4. Tokenize into words (all lower case)\n",
    "        review = review.lower().split()\n",
    "        ## puts all elements of review to lower case then tokenizes (splits into words with delimeter ' ') \n",
    "        \n",
    "        #5. Remove stopwords\n",
    "        eng_stopwords = set(stopwords.words(\"english\"))      \n",
    "        ## stopwords: stuff like 'and' and 'or' and 'are'\n",
    "\n",
    "        clean_review=[]\n",
    "        for word in review: ## iterates through each word in the review\n",
    "            if word not in eng_stopwords: ## does not use stop words in model\n",
    "                if lemmatize is True: ## lemmatize reduces inflected words to english words\n",
    "                    word=wnl.lemmatize(word) \n",
    "                elif stem is True:\n",
    "                    if word == 'oed':  ## skips the rest of loop to avoid known bug\n",
    "                        continue\n",
    "                    word=ps.stem(word) ## stem reduces the word to root, even if its nonenglish\n",
    "                clean_review.append(word)\n",
    "\n",
    "        #6. Join the review to one sentence\n",
    "        \n",
    "        review_processed = ' '.join(clean_review+emoticons)\n",
    "        cleaned_reviews.append(review_processed) ## puts back non-stopwords (and maybe lemmatized/stemmed words)\n",
    "    \n",
    "\n",
    "    return(cleaned_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Function to train and validate a sentiment analysis model using Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# # CountVectorizer can actucally handle a lot of the preprocessing for us\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics # for confusion matrix, accuracy score etc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def train_predict_sentiment(cleaned_reviews, y=train[\"sentiment\"],ngram=1,max_features=1000):\n",
    "    '''This function will:\n",
    "    1. split data into train and test set.\n",
    "    2. get n-gram counts from cleaned reviews \n",
    "    3. train a random forest model using train n-gram counts and y (labels)\n",
    "    4. test the model on your test split\n",
    "    5. print accuracy of sentiment prediction on test and training data\n",
    "    6. print confusion matrix on test data results\n",
    "\n",
    "    To change n-gram type, set value of ngram argument\n",
    "    To change the number of features you want the countvectorizer to generate, set the value of max_features argument'''\n",
    "\n",
    "    print(\"Creating the bag of words model!\\n\")\n",
    "    # CountVectorizer\" is scikit-learn's bag of words tool, here we show more keywords \n",
    "    vectorizer = CountVectorizer(ngram_range=(1, ngram),analyzer = \"word\",   \\\n",
    "                                 tokenizer = None,    \\\n",
    "                                 preprocessor = None, \\\n",
    "                                 stop_words = None,   \\\n",
    "                                 max_features = max_features) \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "    cleaned_reviews, y, random_state=0, test_size=.2)\n",
    "\n",
    "    # Then we use fit_transform() to fit the model / learn the vocabulary,\n",
    "    # then transform the data into feature vectors.\n",
    "    # The input should be a list of strings. .toarraty() converts to a numpy array\n",
    "    \n",
    "    train_bag = vectorizer.fit_transform(X_train).toarray()\n",
    "    test_bag = vectorizer.transform(X_test).toarray()\n",
    "#     print('TOP 20 FEATURES ARE: ',(vectorizer.get_feature_names()[:20]))\n",
    "\n",
    "\n",
    "    print(\"Training the random forest classifier!\\n\")\n",
    "    # Initialize a Random Forest classifier with 75 trees\n",
    "    forest = RandomForestClassifier(n_estimators = 50, random_state=42) \n",
    "\n",
    "    # Fit the forest to the training set, using the bag of words as \n",
    "    # features and the sentiment labels as the target variable\n",
    "    forest = forest.fit(train_bag, y_train)\n",
    "\n",
    "\n",
    "    train_predictions = forest.predict(train_bag)\n",
    "    test_predictions = forest.predict(test_bag)\n",
    "    \n",
    "    train_acc = metrics.accuracy_score(y_train, train_predictions)\n",
    "    valid_acc = metrics.accuracy_score(y_test, test_predictions)\n",
    "    print(\" The training accuracy is: \", train_acc, \"\\n\", \"The validation accuracy is: \", valid_acc)\n",
    "    print()\n",
    "    print('CONFUSION MATRIX:')\n",
    "    print('         Predicted')\n",
    "    print('          neg pos')\n",
    "    print(' Actual')\n",
    "    c=confusion_matrix(y_test, test_predictions)\n",
    "    print('     neg  ',c[0])\n",
    "    print('     pos  ',c[1])\n",
    "\n",
    "    #Extract feature importnace\n",
    "    print('\\nTOP TEN IMPORTANT FEATURES:')\n",
    "    importances = forest.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    top_10 = indices[:10]\n",
    "    print([vectorizer.get_feature_names()[ind] for ind in top_10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train and test  Model on the IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 500 reviews\n",
      "Done with 1000 reviews\n",
      "Done with 1500 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 2500 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 3500 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 4500 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 5500 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 6500 reviews\n",
      "Done with 7000 reviews\n",
      "Done with 7500 reviews\n",
      "Done with 8000 reviews\n",
      "Done with 8500 reviews\n",
      "Done with 9000 reviews\n",
      "Done with 9500 reviews\n",
      "Done with 10000 reviews\n",
      "Done with 10500 reviews\n",
      "Done with 11000 reviews\n",
      "Done with 11500 reviews\n",
      "Done with 12000 reviews\n",
      "Done with 12500 reviews\n",
      "Done with 13000 reviews\n",
      "Done with 13500 reviews\n",
      "Done with 14000 reviews\n",
      "Done with 14500 reviews\n",
      "Done with 15000 reviews\n",
      "Done with 15500 reviews\n",
      "Done with 16000 reviews\n",
      "Done with 16500 reviews\n",
      "Done with 17000 reviews\n",
      "Done with 17500 reviews\n",
      "Done with 18000 reviews\n",
      "Done with 18500 reviews\n",
      "Done with 19000 reviews\n",
      "Done with 19500 reviews\n",
      "Done with 20000 reviews\n",
      "Done with 20500 reviews\n",
      "Done with 21000 reviews\n",
      "Done with 21500 reviews\n",
      "Done with 22000 reviews\n",
      "Done with 22500 reviews\n",
      "Done with 23000 reviews\n",
      "Done with 23500 reviews\n",
      "Done with 24000 reviews\n",
      "Done with 24500 reviews\n",
      "Done with 25000 reviews\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  1.0 \n",
      " The validation accuracy is:  0.8166\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [2103  445]\n",
      "     pos   [ 472 1980]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['bad', 'worst', 'great', 'waste', 'awful', 'excellent', 'best', 'boring', 'poor', 'nothing']\n"
     ]
    }
   ],
   "source": [
    "#Clean the reviews in the training set 'train' using review_cleaner function defined above\n",
    "# Here we use the original reviews without lemmatizing and stemming\n",
    "\n",
    "original_clean_reviews = review_cleaner(\n",
    "    train['review'],\n",
    "    lemmatize = False,\n",
    "    stem = False\n",
    ")\n",
    "\n",
    "train_predict_sentiment(\n",
    "    cleaned_reviews = original_clean_reviews, \n",
    "    y = train[\"sentiment\"],\n",
    "    ngram = 1,\n",
    "    max_features = 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 500 reviews\n",
      "Done with 1000 reviews\n",
      "Done with 1500 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 2500 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 3500 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 4500 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 5500 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 6500 reviews\n",
      "Done with 7000 reviews\n",
      "Done with 7500 reviews\n",
      "Done with 8000 reviews\n",
      "Done with 8500 reviews\n",
      "Done with 9000 reviews\n",
      "Done with 9500 reviews\n",
      "Done with 10000 reviews\n",
      "Done with 10500 reviews\n",
      "Done with 11000 reviews\n",
      "Done with 11500 reviews\n",
      "Done with 12000 reviews\n",
      "Done with 12500 reviews\n",
      "Done with 13000 reviews\n",
      "Done with 13500 reviews\n",
      "Done with 14000 reviews\n",
      "Done with 14500 reviews\n",
      "Done with 15000 reviews\n",
      "Done with 15500 reviews\n",
      "Done with 16000 reviews\n",
      "Done with 16500 reviews\n",
      "Done with 17000 reviews\n",
      "Done with 17500 reviews\n",
      "Done with 18000 reviews\n",
      "Done with 18500 reviews\n",
      "Done with 19000 reviews\n",
      "Done with 19500 reviews\n",
      "Done with 20000 reviews\n",
      "Done with 20500 reviews\n",
      "Done with 21000 reviews\n",
      "Done with 21500 reviews\n",
      "Done with 22000 reviews\n",
      "Done with 22500 reviews\n",
      "Done with 23000 reviews\n",
      "Done with 23500 reviews\n",
      "Done with 24000 reviews\n",
      "Done with 24500 reviews\n",
      "Done with 25000 reviews\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  1.0 \n",
      " The validation accuracy is:  0.8216\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [2110  438]\n",
      "     pos   [ 454 1998]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['bad', 'worst', 'great', 'waste', 'awful', 'excellent', 'boring', 'terrible', 'nothing', 'best']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_clean_reviews = review_cleaner(\n",
    "    train['review'],\n",
    "    lemmatize = True,\n",
    "    stem = False\n",
    ")\n",
    "\n",
    "train_predict_sentiment(\n",
    "    cleaned_reviews = lemmatized_clean_reviews, \n",
    "    y = train[\"sentiment\"],\n",
    "    ngram = 1,\n",
    "    max_features = 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 500 reviews\n",
      "Done with 1000 reviews\n",
      "Done with 1500 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 2500 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 3500 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 4500 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 5500 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 6500 reviews\n"
     ]
    }
   ],
   "source": [
    "stemmed_clean_reviews = review_cleaner(\n",
    "    train['review'],\n",
    "    lemmatize = False,\n",
    "    stem = True\n",
    ")\n",
    "\n",
    "train_predict_sentiment(\n",
    "    cleaned_reviews = stemmed_clean_reviews, \n",
    "    y = train[\"sentiment\"],\n",
    "    ngram = 1,\n",
    "    max_features = 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_clean_reviews = review_cleaner(\n",
    "    train['review'],\n",
    "    lemmatize = False,\n",
    "    stem = False\n",
    ")\n",
    "\n",
    "train_predict_sentiment(\n",
    "    cleaned_reviews = original_clean_reviews, \n",
    "    y = train[\"sentiment\"],\n",
    "    ngram = 2,\n",
    "    max_features = 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_2_clean_reviews = review_cleaner(\n",
    "    train['review'],\n",
    "    lemmatize = True,\n",
    "    stem = False\n",
    ")\n",
    "\n",
    "train_predict_sentiment(\n",
    "    cleaned_reviews = lemmatized_2_clean_reviews, \n",
    "    y = train[\"sentiment\"],\n",
    "    ngram = 2,\n",
    "    max_features = 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_2_clean_reviews = review_cleaner(\n",
    "    train['review'],\n",
    "    lemmatize = False,\n",
    "    stem = True\n",
    ")\n",
    "\n",
    "train_predict_sentiment(\n",
    "    cleaned_reviews = stemmed_2_clean_reviews, \n",
    "    y = train[\"sentiment\"],\n",
    "    ngram = 2,\n",
    "    max_features = 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict_sentiment(\n",
    "    cleaned_reviews = lemmatized_clean_reviews, \n",
    "    y = train[\"sentiment\"],\n",
    "    ngram = 1,\n",
    "    max_features = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict_sentiment(\n",
    "    cleaned_reviews = lemmatized_clean_reviews, \n",
    "    y = train[\"sentiment\"],\n",
    "    ngram = 1,\n",
    "    max_features = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict_sentiment(\n",
    "    cleaned_reviews = lemmatized_clean_reviews, \n",
    "    y = train[\"sentiment\"],\n",
    "    ngram = 1,\n",
    "    max_features = 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict_sentiment(\n",
    "    cleaned_reviews = lemmatized_clean_reviews, \n",
    "    y = train[\"sentiment\"],\n",
    "    ngram = 1,\n",
    "    max_features = 5000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. TODO: \n",
    "\n",
    "To do this exercise you only need to change argument values in the functions review_cleaner() and train_predict_semtiment(). Go through the functions to understand what they do. Perform the following -\n",
    "\n",
    "1. For __UNIGRAM setting__ ie. when ngram=1 in the function `train_predict_sentiment()`, compare the performance of original cleaned reviews in Sentiment anlysis to -  \n",
    "    1. lemmatized reviews\n",
    "    2. stemmed reviews\n",
    "2. For __BIGRAM setting__ ie. when ngram=2 in the function `train_predict_sentiment()`, compare the performance of original cleaned reviews in sentiment analysis to:\n",
    "     1. lemmatized reviews\n",
    "     2. stemmed reviews\n",
    "3. For __UNIGRAM setting__ ie. ngram=1 and lemmatize = True , compare the performance of Sentiment analysis for these different values of maximum features = [10,100,1000,5000], you can change the value of argument max_features in `train_predict_sentiment()\n",
    "    \n",
    "### SUBMISSION:  For each question in 5. TODO  report your results in a PDF. \n",
    "\n",
    "### Mention the  review_cleaner( ) and train_predict_sentiment( ) argument setting that you used in each case. Do not submit any ipython notebook.\n",
    "\n",
    "Example : For original review with unigram and 5000 max_features, I will report:\n",
    "\n",
    "``` original_clean_reviews=review_cleaner(train['review'],lemmatize=False,stem=False)\n",
    "train_predict_sentiment(cleaned_reviews=original_clean_reviews, y=train[\"sentiment\"],ngram=1,max_features=5000)\n",
    "\n",
    "The training accuracy is:  1.0 \n",
    "The validation accuracy is:  0.836 ```\n",
    "\n",
    "## Also write a 100-200 word summary of your observations overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# __Homework 8:__ Natural Language Processing\n",
    "\n",
    "\n",
    "_Name_: Casey Chadwell  \n",
    "_SID_: 3033291861   \n",
    "_Class_: IEOR 135\n",
    "_GitHub_:    \n",
    "\n",
    "---\n",
    "\n",
    "__Preliminaries__: For this assignment, I began by setting the parameter `random_state` to `42` in the random forest classifier used in the `train_predict_sentiment` function. This was mentioned on Piazza as a good idea to ensure the results I obtain are replicable. The changed line is as follows:\n",
    "\n",
    "`forest = RandomForestClassifier(n_estimators = 50, random_state=42)`\n",
    "\n",
    "For each part, I changed the function name `original_clean_reviews` to something more appropriate for the task at hand for the sake of clarity. (e.g. `lemmatized_clean_reviews`)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. __Unigram Setting__\n",
    "For original reviews with unigram and 1000 max features, the functions were defined as follows:\n",
    "\n",
    "```\n",
    "original_clean_reviews=review_cleaner(\n",
    "    train['review'],\n",
    "    lemmatize = False,\n",
    "    stem = False\n",
    ")\n",
    "\n",
    "train_predict_sentiment(\n",
    "    cleaned_reviews = original_clean_reviews,\n",
    "    y = train[\"sentiment\"],\n",
    "    ngram = 1,\n",
    "    max_features = 1000\n",
    ")\n",
    "```\n",
    "\n",
    "The original cleaning settings produced the following accuracies from the random forest classifier:\n",
    "\n",
    "> The training accuracy is:  1.0   \n",
    "> The validation accuracy is:  0.8166\n",
    "\n",
    "---\n",
    "\n",
    "#### __A. lemmatized reviews__:\n",
    "\n",
    "To create lemmatized reviews, the parameter `lemmatize` in the function `original_clean_reviews` was set to `True` and the parameter `stem` was left as `False`. The function calls were as follows:\n",
    "\n",
    "```\n",
    "lemmatized_clean_reviews = review_cleaner(\n",
    "    train['review'],\n",
    "    lemmatize = True,\n",
    "    stem = False\n",
    ")\n",
    "\n",
    "train_predict_sentiment(\n",
    "    cleaned_reviews = lemmatized_clean_reviews,\n",
    "    y = train[\"sentiment\"],\n",
    "    ngram = 1,\n",
    "    max_features = 1000\n",
    ")\n",
    "```\n",
    "Lemmatizing the reviews produced the following accuracies:\n",
    "\n",
    "> The training accuracy is:  1.0   \n",
    "> The validation accuracy is:  0.8216\n",
    "\n",
    "__Brief Overview__:\n",
    "Lemmatizing the reviews resulted in a 0.005 increase in validation accuracy while maintaining a perfect 100% training accuracy.\n",
    "\n",
    "#### __B. Stemmed Reviews__:\n",
    "\n",
    "To create stemmed reviews, the parameter `lemmatize` in the function `original_clean_reviews` was set to `False` and the parameter `stem` was set to `True`. The function calls were as follows:\n",
    "\n",
    "```\n",
    "stemmed_clean_reviews = review_cleaner(\n",
    "    train['review'],\n",
    "    lemmatize = False,\n",
    "    stem = True\n",
    ")\n",
    "\n",
    "train_predict_sentiment(\n",
    "    cleaned_reviews = stemmed_clean_reviews,\n",
    "    y = train[\"sentiment\"],\n",
    "    ngram = 1,\n",
    "    max_features = 1000\n",
    ")\n",
    "```\n",
    "Stemming the reviews produced the following accuracies:\n",
    "\n",
    "> The training accuracy is:  1.0    \n",
    "> The validation accuracy is:  0.8196\n",
    "\n",
    "__Brief Overview__:\n",
    "Stemming the reviews resulted in a 0.003 increase in validation accuracy while also maintaining a perfect training accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. __Bigram Setting__\n",
    "\n",
    "For this part, the results were obtained by changing the parameter `ngram` from `1` to `2` in the `train_predict_sentiment` function.\n",
    "\n",
    "---\n",
    "\n",
    "#### __A. lemmatized reviews__:\n",
    "\n",
    "In the call to `original_clean_reviews`, `lemmatized` was set to `True` and `stem` was set to `False`. The parameter `ngram` was changed to `2`. The function calls were as follows:\n",
    "\n",
    "```\n",
    "lemmatized_2_clean_reviews = review_cleaner(\n",
    "    train['review'],\n",
    "    lemmatize = True,\n",
    "    stem = False\n",
    ")\n",
    "\n",
    "train_predict_sentiment(\n",
    "    cleaned_reviews = lemmatized_2_clean_reviews,\n",
    "    y = train[\"sentiment\"],\n",
    "    ngram = 2,\n",
    "    max_features = 1000\n",
    ")\n",
    "```\n",
    "Lemmatizing the reviews and increasing `ngram` to 2 produced the following accuracies:\n",
    "\n",
    "> The training accuracy is:  1.0   \n",
    "> The validation accuracy is:  0.8196\n",
    "\n",
    "__Brief Overview__:\n",
    "Increasing `ngram` to 2 produced a slight 0.003 increase in validation accuracy compared to the original cleaning function, however the validation accuracy is lower than with `ngram = 1` for lemmatized reviews.\n",
    "\n",
    "#### __B. Stemmed Reviews__:\n",
    "\n",
    "In the call to `original_clean_reviews`, `lemmatized` was set to `False` and `stem` was set to `True`. The parameter `ngram` was set to `2`. The function calls were as follows:\n",
    "\n",
    "```\n",
    "stemmed_2_clean_reviews = review_cleaner(\n",
    "    train['review'],\n",
    "    lemmatize = False,\n",
    "    stem = True\n",
    ")\n",
    "\n",
    "train_predict_sentiment(\n",
    "    cleaned_reviews = stemmed_2_clean_reviews,\n",
    "    y = train[\"sentiment\"],\n",
    "    ngram = 2,\n",
    "    max_features = 1000\n",
    ")\n",
    "```\n",
    "Lemmatizing the reviews and increasing `ngram` to 2 produced the following accuracies:\n",
    "\n",
    "> The training accuracy is:  1.0    \n",
    "> The validation accuracy is:  0.8196\n",
    "\n",
    "__Brief Overview__:\n",
    "Stemming with `ngram = 2` produced the same validation accuracy as stemming with `ngram = 1`.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. __Max Features__: with Unigram Setting\n",
    "\n",
    "For this part, the results were obtained by using lemmatized reviews with `ngram=1`. In the following parts, the review cleaning function call was left as:\n",
    "\n",
    "```\n",
    "lemmatized_clean_reviews = review_cleaner(\n",
    "    train['review'],\n",
    "    lemmatize = True,\n",
    "    stem = False\n",
    ")\n",
    "```\n",
    "\n",
    "In the `train_predict_sentiment` function, the value of `max_features` was changed to 10, 100, 1000, then 5000. The following accuracies were produced:\n",
    "\n",
    "#### `max_features= 10`:\n",
    "\n",
    "> The training accuracy is:  0.8715     \n",
    "> The validation accuracy is:  0.5604\n",
    "\n",
    "#### `max_features= 100`:\n",
    "\n",
    "> The training accuracy is:  0.9999       \n",
    "> The validation accuracy is:  0.7212\n",
    "\n",
    "#### `max_features= 1000`:\n",
    "\n",
    "> The training accuracy is:  1.0     \n",
    "> The validation accuracy is:  0.8216\n",
    "\n",
    "#### `max_features= 5000`:\n",
    "\n",
    "> The training accuracy is:  1.0       \n",
    "> The validation accuracy is:  0.8384\n",
    "\n",
    "__Brief Overview__:\n",
    "Increasing the `max_features` parameter seems to increase the accuracy of the model, however that increase slows as n increases.\n",
    "\n",
    "---\n",
    "\n",
    "# Overall Summary\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
